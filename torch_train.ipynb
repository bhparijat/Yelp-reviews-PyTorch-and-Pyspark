{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import time\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import string\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epochs = 10\n",
    "num_class = 2\n",
    "embed_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"gpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewsSentimentAnalysis(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_dim,num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,json_file,threshold=3):\n",
    "        \n",
    "        self.raw_data = pd.read_json(json_file,lines=True)\n",
    "    \n",
    "        self.raw_data['label'] = self.raw_data.stars.apply(lambda x : 1 if x>=3 else 0)\n",
    "        \n",
    "        self.raw_data = self.raw_data[[\"label\",\"text\"]].iloc[:1000,]\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        \n",
    "        self.idx2word = {}\n",
    "        \n",
    "        self.word2freq = {}\n",
    "        self.word_count = 0\n",
    "        \n",
    "        self.maxLen = 0\n",
    "        self.__init__preprocess()\n",
    "        \n",
    "        self.data = self.raw_data.to_numpy()\n",
    "   \n",
    "        \n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        \n",
    "        sample = self.data[idx,:]\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __init__preprocess(self):\n",
    "        \n",
    "        \n",
    "        def clean(text):\n",
    "            text = text.lower()\n",
    "        \n",
    "            text = [ch for ch in text if ch not in string.punctuation]\n",
    "\n",
    "\n",
    "            text = \"\".join(text)\n",
    "\n",
    "            text = [c for c in text if c == \" \" or c.isalnum()]\n",
    "\n",
    "            text = \"\".join(text)\n",
    "\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "            text = text.split(\" \")\n",
    "\n",
    "            text = [word for word in text if word not in stop_words]\n",
    "        \n",
    "        \n",
    "            text = \" \".join(text)\n",
    "            \n",
    "            return text\n",
    "          \n",
    "            \n",
    "        def build_vocab(text):\n",
    "            \n",
    "            \n",
    "            text = text.split(\" \")\n",
    "            \n",
    "            text_token = []\n",
    "            \n",
    "            for word in text:\n",
    "                \n",
    "                if word not in self.word2idx:\n",
    "                    \n",
    "                    self.word2idx[word] = self.word_count\n",
    "                    \n",
    "                    self.idx2word[self.word_count] = word\n",
    "                    \n",
    "                    self.word_count+=1\n",
    "                    \n",
    "                    \n",
    "                text_token.append(self.word2idx[word])\n",
    "             \n",
    "            self.maxLen = max(self.maxLen,len(text_token))\n",
    "            return text_token\n",
    "        \n",
    "        self.raw_data['text'] = self.raw_data.text.apply(lambda x : clean(x))\n",
    "        self.raw_data['text'] = self.raw_data.text.apply(lambda x : build_vocab(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_offset(batch):\n",
    "    \n",
    "    text = [torch.tensor(x[1]) for x in batch]\n",
    "    \n",
    "    label = torch.tensor([x[0] for x in batch])\n",
    "    offsets = [0] + [len(x) for x in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    \n",
    "    text = torch.cat(text)\n",
    "    \n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(train):\n",
    "    \n",
    "    train_data = DataLoader(train,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_offset,num_workers=4)\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for i, (text, offsets, label) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, label = text.to(device), offsets.to(device), label.to(device)\n",
    "        \n",
    "    \n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss / len(train), train_acc / len(train)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(test):\n",
    "    \n",
    "    test_data = DataLoader(test,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_offset,num_workers=4)\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    for i, (text, offsets, label) in enumerate(train_data):\n",
    "        \n",
    "        text, offsets, label = text.to(device), offsets.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            output = model(text, offsets)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            test_acc += (output.argmax(1) == label).sum().item()   \n",
    "    \n",
    "    return test_loss / len(test), test_acc / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to for dataset preprocessing is 2.0134612878163654 minutes...\n",
      "Length of training dataset is 800\n",
      "Length of validation dataset is 100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Subset' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c0ad34ad3e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Subset' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    yelp_dataset = YelpDataset(json_file = \"~/data/yelp/review.json\")\n",
    "    \n",
    "    print(\"Time Taken to for dataset preprocessing is {} minutes...\".format((time.time()-start_time)/60))\n",
    "    \n",
    "    \n",
    "    train_len = int(len(yelp_dataset)*0.8)\n",
    "    \n",
    "    valid_len = int(len(yelp_dataset)*0.1)\n",
    "    \n",
    "    test_len = len(yelp_dataset) - train_len -valid_len\n",
    "    \n",
    "    train,valid,test = random_split(yelp_dataset,[train_len,valid_len,test_len])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Length of training dataset is {}\".format(train_len))\n",
    "          \n",
    "    print(\"Length of validation dataset is {}\".format(valid_len))\n",
    "          \n",
    "      \n",
    "    model = YelpReviewsSentimentAnalysis(yelp_dataset.word_count,embed_dim, num_class)\n",
    "    \n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "     \n",
    "    training_loss,training_acc = [],[]\n",
    "    validation_loss,validation_acc = [],[]\n",
    "    \n",
    "    for epch in range(epochs):\n",
    "          \n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train_func(train)\n",
    "        valid_loss, valid_acc = test(valid)\n",
    "\n",
    "        training_loss.append(train_loss)\n",
    "        training_acc.append(train_acc)\n",
    "        \n",
    "        \n",
    "        validation_loss.append(valid_loss)\n",
    "        validation_acc.append(valid_acc)\n",
    "        \n",
    "        \n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        \n",
    "        print(\"Time Taken to for dataset preprocessing is {} minutes...\".format(mins))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(training_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
