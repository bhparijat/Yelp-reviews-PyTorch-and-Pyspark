{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import time\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import string\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from torch.autograd import Variable\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epochs = 10\n",
    "num_class = 2\n",
    "embed_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"gpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewsSentimentAnalysis(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_dim,num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):w\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,json_file,threshold=3):\n",
    "        \n",
    "        self.raw_data = pd.read_json(json_file,lines=True)\n",
    "    \n",
    "        self.raw_data['label'] = self.raw_data.stars.apply(lambda x : 1 if x>=3 else 0)\n",
    "        \n",
    "        self.raw_data = self.raw_data[[\"label\",\"text\"]].iloc[:100000,]\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        \n",
    "        self.idx2word = {}\n",
    "        \n",
    "        self.word2freq = {}\n",
    "        self.word_count = 0\n",
    "        \n",
    "        self.maxLen = 0\n",
    "        self.__init__preprocess()\n",
    "        \n",
    "        self.data = self.raw_data.to_numpy()\n",
    "   \n",
    "        \n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        \n",
    "        sample = self.data[idx,:]\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __init__preprocess(self):\n",
    "        \n",
    "        \n",
    "        def clean(text):\n",
    "            text = text.lower()\n",
    "        \n",
    "            text = [ch for ch in text if ch not in string.punctuation]\n",
    "\n",
    "\n",
    "            text = \"\".join(text)\n",
    "\n",
    "            text = [c for c in text if c == \" \" or c.isalnum()]\n",
    "\n",
    "            text = \"\".join(text)\n",
    "\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "            text = text.split(\" \")\n",
    "\n",
    "            text = [word for word in text if word not in stop_words]\n",
    "        \n",
    "        \n",
    "            text = \" \".join(text)\n",
    "            \n",
    "            return text\n",
    "          \n",
    "            \n",
    "        def build_vocab(text):\n",
    "            \n",
    "            \n",
    "            text = text.split(\" \")\n",
    "            \n",
    "            text_token = []\n",
    "            \n",
    "            for word in text:\n",
    "                \n",
    "                if word not in self.word2idx:\n",
    "                    \n",
    "                    self.word2idx[word] = self.word_count\n",
    "                    \n",
    "                    self.idx2word[self.word_count] = word\n",
    "                    \n",
    "                    self.word_count+=1\n",
    "                    \n",
    "                    \n",
    "                text_token.append(self.word2idx[word])\n",
    "             \n",
    "            self.maxLen = max(self.maxLen,len(text_token))\n",
    "            return text_token\n",
    "        \n",
    "        self.raw_data['text'] = self.raw_data.text.apply(lambda x : clean(x))\n",
    "        self.raw_data['text'] = self.raw_data.text.apply(lambda x : build_vocab(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_offset(batch):\n",
    "    \n",
    "    text = [torch.tensor(x[1]) for x in batch]\n",
    "    \n",
    "    label = torch.tensor([x[0] for x in batch])\n",
    "    offsets = [0] + [len(x) for x in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    \n",
    "    text = torch.cat(text)\n",
    "    \n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(train):\n",
    "    \n",
    "    train_data = DataLoader(train,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_offset,num_workers=4)\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for i, (text, offsets, label) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, label = text.to(device), offsets.to(device), label.to(device)\n",
    "        \n",
    "    \n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        loss = Variable(loss,requires_grad=True)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss / len(train), train_acc / len(train)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(test):\n",
    "    \n",
    "    test_data = DataLoader(test,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_offset,num_workers=4)\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    for i, (text, offsets, label) in enumerate(test_data):\n",
    "        \n",
    "        text, offsets, label = text.to(device), offsets.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            output = model(text, offsets)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss = Variable(loss,requires_grad=True)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            test_acc += (output.argmax(1) == label).sum().item()   \n",
    "    \n",
    "    return test_loss / len(test), test_acc / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to for dataset preprocessing is 2.8071921269098916 minutes...\n",
      "Length of training dataset is 80000\n",
      "Length of validation dataset is 10000\n",
      "{} saved.. model_0.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_1.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_2.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_3.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_4.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_5.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_6.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_7.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_8.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n",
      "{} saved.. model_9.pkl\n",
      "Time Taken to complete epoch is 0.13333333333333333 minutes...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    yelp_dataset = YelpDataset(json_file = \"~/data/yelp/review.json\")\n",
    "    \n",
    "    print(\"Time Taken to for dataset preprocessing is {} minutes...\".format((time.time()-start_time)/60))\n",
    "    \n",
    "    \n",
    "    train_len = int(len(yelp_dataset)*0.8)\n",
    "    \n",
    "    valid_len = int(len(yelp_dataset)*0.1)\n",
    "    \n",
    "    test_len = len(yelp_dataset) - train_len -valid_len\n",
    "    \n",
    "    train,valid,test = random_split(yelp_dataset,[train_len,valid_len,test_len])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Length of training dataset is {}\".format(train_len))\n",
    "          \n",
    "    print(\"Length of validation dataset is {}\".format(valid_len))\n",
    "          \n",
    "      \n",
    "    model = YelpReviewsSentimentAnalysis(yelp_dataset.word_count,embed_dim, num_class)\n",
    "    \n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "     \n",
    "    tl_,ta_ = [],[]\n",
    "    vl_,va_ = [],[]\n",
    "    \n",
    "    for epch in range(epochs):\n",
    "          \n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train_func(train)\n",
    "        valid_loss, valid_acc = test_func(valid)\n",
    "\n",
    "        tl_.append(train_loss)\n",
    "        ta_.append(train_acc)\n",
    "        \n",
    "        \n",
    "        vl_.append(valid_loss)\n",
    "        va_.append(valid_acc)\n",
    "        \n",
    "        \n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        \n",
    "        \n",
    "        \n",
    "        name = \"model_\"+str(epch)+\".pkl\"\n",
    "        with open(name,\"wb\") as file:\n",
    "            pkl.dump(model,file)\n",
    "            \n",
    "        print(\"{} saved..\",name)\n",
    "        print(\"Time Taken to complete epoch is {} minutes...\".format(mins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.505725,\n",
       " 0.505725,\n",
       " 0.505725,\n",
       " 0.505725,\n",
       " 0.505725,\n",
       " 0.505725,\n",
       " 0.505725,\n",
       " 0.505725,\n",
       " 0.505725,\n",
       " 0.505725]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5098,\n",
       " 0.5098,\n",
       " 0.5098,\n",
       " 0.5098,\n",
       " 0.5098,\n",
       " 0.5098,\n",
       " 0.5098,\n",
       " 0.5098,\n",
       " 0.5098,\n",
       " 0.5098]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156315"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_dataset.word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.7731e-01,  3.7013e-01, -7.6838e-02, -4.5141e-01,  1.9208e-01,\n",
       "          3.9211e-01,  4.8415e-02,  1.7409e-01,  4.2344e-01,  8.9563e-03,\n",
       "         -2.9832e-01, -1.1177e-01, -4.4683e-01, -1.8006e-01,  7.3528e-02,\n",
       "         -4.3666e-01,  3.7348e-01,  1.5231e-01, -9.3723e-02, -1.8647e-01,\n",
       "          2.7397e-01,  2.6460e-02,  4.5881e-01,  3.3134e-01,  4.4123e-01,\n",
       "         -2.7587e-01, -3.9694e-01,  2.6389e-01,  2.1557e-01,  2.1427e-01,\n",
       "          4.8920e-01,  2.9288e-01,  2.3015e-01,  1.3968e-01, -3.8236e-01,\n",
       "          2.7120e-01, -3.7601e-01,  9.5279e-02,  2.0113e-01, -2.3488e-01,\n",
       "          2.9296e-01, -4.9180e-02, -3.6096e-01, -1.6072e-03, -2.0295e-02,\n",
       "         -3.5595e-01, -2.2731e-01, -9.5978e-03, -1.2998e-01, -3.7972e-01,\n",
       "         -2.4652e-02,  5.9012e-02, -2.2972e-01,  3.5491e-01, -2.8106e-01,\n",
       "         -2.6579e-01, -2.1652e-01, -4.3964e-01, -3.4342e-01, -4.7103e-01,\n",
       "          9.5100e-02,  4.5640e-01,  2.1777e-01,  2.4502e-01,  2.0616e-01,\n",
       "          1.8029e-01,  4.7376e-01, -1.9221e-01, -4.0725e-01,  2.4264e-01,\n",
       "         -2.0293e-01, -1.0206e-01, -8.9532e-02,  3.2491e-01, -4.0899e-01,\n",
       "          1.1427e-01, -2.6247e-01, -3.7747e-01,  3.0074e-01, -4.0371e-01,\n",
       "          2.1729e-01,  3.7128e-01, -3.8721e-01,  4.1815e-01, -1.2687e-01,\n",
       "         -3.7019e-01,  1.6894e-01,  3.0542e-01, -3.9238e-02, -1.7706e-01,\n",
       "          1.6566e-01, -3.0989e-01,  2.2575e-01, -4.9335e-01, -4.8640e-01,\n",
       "          4.3218e-01, -2.2946e-01,  2.9993e-04, -3.6907e-01, -3.7292e-01],\n",
       "        [ 1.0478e-01, -1.1745e-01,  8.5806e-02, -3.2074e-01,  9.9453e-02,\n",
       "          2.7683e-02, -3.2318e-01, -4.9392e-01,  4.1011e-02, -2.8598e-01,\n",
       "          3.0828e-01,  3.6362e-01, -2.0776e-01, -3.5356e-01, -3.0675e-01,\n",
       "         -3.3329e-01, -7.2317e-02,  4.2541e-01, -3.6423e-01, -2.6364e-01,\n",
       "          3.7363e-01, -2.8127e-01,  2.7428e-01, -2.4632e-01,  2.8207e-01,\n",
       "          5.2618e-02,  3.9154e-01, -1.3622e-01, -3.9116e-01,  6.4240e-02,\n",
       "          5.9622e-02,  1.9766e-01,  1.8988e-01, -8.1621e-03,  3.7863e-01,\n",
       "         -4.1634e-03,  2.8976e-01,  1.5513e-01,  2.4094e-01, -4.8657e-01,\n",
       "         -1.1401e-01,  3.4640e-01, -3.6922e-01,  1.3188e-02, -1.5608e-02,\n",
       "         -1.2810e-01,  2.7960e-01,  1.0678e-01,  2.5307e-01, -2.6034e-02,\n",
       "         -1.8674e-01,  6.4677e-02, -3.0793e-01, -4.0132e-01, -1.8577e-01,\n",
       "         -4.8431e-01, -2.6266e-01, -4.8546e-01,  2.2705e-01, -2.4301e-02,\n",
       "         -3.7142e-01,  4.2267e-01, -2.8686e-01, -2.1727e-01,  2.1598e-01,\n",
       "         -2.1406e-01,  2.4215e-01, -2.9444e-01,  3.2729e-01,  3.1986e-01,\n",
       "          1.6851e-01, -3.6174e-01, -2.4451e-01,  4.7086e-01,  4.6831e-01,\n",
       "         -5.6721e-02,  1.7518e-01,  2.0892e-02,  2.3194e-01,  3.0132e-01,\n",
       "          4.7272e-01,  1.9883e-01,  2.0358e-01, -3.8569e-02,  9.6371e-03,\n",
       "          1.5563e-01,  2.5363e-01,  4.7704e-01, -2.5803e-01, -2.0087e-01,\n",
       "         -4.9265e-01,  1.7042e-01, -3.7833e-01, -8.6359e-02, -3.7364e-01,\n",
       "          4.2949e-02,  2.2072e-01, -4.7965e-03, -1.5298e-01,  3.8480e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_1.pkl\",\"rb\") as file:\n",
    "    model_1 = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_9.pkl\",\"rb\") as file:\n",
    "    model_9 = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.3565e-01,  3.0212e-02,  2.9023e-01,  3.5425e-01, -3.6518e-01,\n",
       "          2.1126e-01,  3.4472e-01,  4.5807e-01,  2.2894e-01, -2.3293e-01,\n",
       "         -1.0985e-01,  1.6811e-01, -2.5006e-01,  4.1443e-01, -1.6038e-01,\n",
       "         -3.8092e-01,  2.6662e-01,  1.5123e-01, -8.5601e-02, -3.4376e-01,\n",
       "         -2.9098e-01,  2.1467e-01,  1.1533e-01,  3.5420e-01, -5.0050e-04,\n",
       "         -1.9033e-01, -3.0156e-01,  4.3343e-01,  2.4082e-01, -3.8623e-01,\n",
       "         -2.7676e-01,  8.2076e-02, -1.6928e-01, -3.8063e-01, -4.3473e-01,\n",
       "         -3.2350e-02, -4.1604e-05,  4.6637e-01, -1.1415e-01,  3.0355e-01,\n",
       "          1.5025e-01,  1.0685e-01,  2.7617e-01, -7.3167e-03,  3.0006e-02,\n",
       "          3.5998e-01,  1.7222e-01, -1.3075e-01,  3.9354e-01, -2.7977e-01,\n",
       "          5.0323e-02, -2.4725e-01,  3.3777e-01, -3.7121e-01, -1.1962e-01,\n",
       "         -3.5043e-01,  1.6030e-01,  4.9341e-01,  1.1727e-02, -2.6584e-01,\n",
       "         -2.1087e-01,  3.0885e-01, -3.5393e-02,  4.8703e-01,  7.8680e-02,\n",
       "         -4.2900e-01,  3.3733e-01,  1.6053e-01,  9.9290e-02, -3.4958e-01,\n",
       "          6.9115e-02,  1.1750e-01,  4.9842e-01,  3.3440e-01, -3.7567e-01,\n",
       "         -2.3261e-02,  2.2686e-01, -1.0687e-01, -3.1337e-01, -1.3214e-01,\n",
       "         -4.0789e-01,  6.8949e-03, -6.3707e-02, -1.7981e-01, -2.2906e-01,\n",
       "         -4.4337e-01,  4.2156e-01,  2.5767e-01,  4.2724e-01, -3.3223e-01,\n",
       "         -2.0813e-01, -4.2437e-01, -1.9534e-01, -4.3503e-01,  4.7066e-01,\n",
       "         -4.8278e-01,  2.2643e-01, -2.3322e-01, -2.3575e-01, -2.5133e-01],\n",
       "        [ 1.7660e-01, -4.5588e-01,  1.2693e-01, -6.7996e-02,  1.8640e-01,\n",
       "          4.5978e-01,  1.5470e-01,  1.2402e-01,  6.8502e-03,  2.4248e-01,\n",
       "          2.7229e-01,  1.6465e-01,  3.7472e-01,  1.6022e-01, -4.3188e-01,\n",
       "          2.1902e-01, -1.3178e-01,  4.7777e-01,  1.2327e-01,  3.1905e-01,\n",
       "          1.7882e-02, -4.7733e-01, -4.0570e-01, -9.7906e-02, -3.9830e-01,\n",
       "         -3.2315e-01,  4.5852e-01,  5.6298e-02, -4.6132e-01,  6.3126e-02,\n",
       "         -4.4313e-01, -4.3523e-02, -2.1989e-01,  1.5554e-01, -2.8521e-01,\n",
       "         -1.6950e-01,  4.7483e-02,  1.4223e-01, -4.5080e-01, -2.6366e-01,\n",
       "         -1.5620e-01,  2.4166e-01,  4.4282e-01, -8.2939e-02,  1.1997e-01,\n",
       "         -1.7720e-02,  3.6912e-01,  3.9332e-01,  1.6665e-01,  7.9002e-02,\n",
       "         -1.5330e-01, -1.6126e-01,  3.7063e-01,  7.0298e-02,  1.4623e-01,\n",
       "         -2.4693e-01,  2.5721e-02, -2.3281e-01, -3.1482e-01,  1.4820e-01,\n",
       "          4.2068e-01,  3.2337e-01,  1.6090e-01,  4.8289e-01, -1.3437e-01,\n",
       "         -1.3357e-01, -4.0732e-01,  3.8818e-01, -4.2529e-01, -3.7258e-01,\n",
       "          3.7369e-01, -5.0266e-02, -1.0642e-01,  2.8776e-01,  2.2167e-01,\n",
       "          1.2928e-01, -5.2390e-02, -3.7240e-01, -3.5484e-03,  3.6238e-01,\n",
       "         -4.0364e-02,  2.6802e-01,  1.1419e-01, -7.1792e-02, -4.3012e-01,\n",
       "          3.9256e-01, -1.8475e-01, -4.5062e-01,  1.9000e-01, -1.8650e-04,\n",
       "         -2.0112e-01, -1.5058e-01, -4.3190e-04, -5.1187e-02, -1.6124e-01,\n",
       "         -3.9582e-01,  3.8025e-01, -4.7854e-01, -8.4481e-02, -1.9682e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3565e-01,  3.0212e-02,  2.9023e-01,  3.5425e-01, -3.6518e-01,\n",
       "          2.1126e-01,  3.4472e-01,  4.5807e-01,  2.2894e-01, -2.3293e-01,\n",
       "         -1.0985e-01,  1.6811e-01, -2.5006e-01,  4.1443e-01, -1.6038e-01,\n",
       "         -3.8092e-01,  2.6662e-01,  1.5123e-01, -8.5601e-02, -3.4376e-01,\n",
       "         -2.9098e-01,  2.1467e-01,  1.1533e-01,  3.5420e-01, -5.0050e-04,\n",
       "         -1.9033e-01, -3.0156e-01,  4.3343e-01,  2.4082e-01, -3.8623e-01,\n",
       "         -2.7676e-01,  8.2076e-02, -1.6928e-01, -3.8063e-01, -4.3473e-01,\n",
       "         -3.2350e-02, -4.1604e-05,  4.6637e-01, -1.1415e-01,  3.0355e-01,\n",
       "          1.5025e-01,  1.0685e-01,  2.7617e-01, -7.3167e-03,  3.0006e-02,\n",
       "          3.5998e-01,  1.7222e-01, -1.3075e-01,  3.9354e-01, -2.7977e-01,\n",
       "          5.0323e-02, -2.4725e-01,  3.3777e-01, -3.7121e-01, -1.1962e-01,\n",
       "         -3.5043e-01,  1.6030e-01,  4.9341e-01,  1.1727e-02, -2.6584e-01,\n",
       "         -2.1087e-01,  3.0885e-01, -3.5393e-02,  4.8703e-01,  7.8680e-02,\n",
       "         -4.2900e-01,  3.3733e-01,  1.6053e-01,  9.9290e-02, -3.4958e-01,\n",
       "          6.9115e-02,  1.1750e-01,  4.9842e-01,  3.3440e-01, -3.7567e-01,\n",
       "         -2.3261e-02,  2.2686e-01, -1.0687e-01, -3.1337e-01, -1.3214e-01,\n",
       "         -4.0789e-01,  6.8949e-03, -6.3707e-02, -1.7981e-01, -2.2906e-01,\n",
       "         -4.4337e-01,  4.2156e-01,  2.5767e-01,  4.2724e-01, -3.3223e-01,\n",
       "         -2.0813e-01, -4.2437e-01, -1.9534e-01, -4.3503e-01,  4.7066e-01,\n",
       "         -4.8278e-01,  2.2643e-01, -2.3322e-01, -2.3575e-01, -2.5133e-01],\n",
       "        [ 1.7660e-01, -4.5588e-01,  1.2693e-01, -6.7996e-02,  1.8640e-01,\n",
       "          4.5978e-01,  1.5470e-01,  1.2402e-01,  6.8502e-03,  2.4248e-01,\n",
       "          2.7229e-01,  1.6465e-01,  3.7472e-01,  1.6022e-01, -4.3188e-01,\n",
       "          2.1902e-01, -1.3178e-01,  4.7777e-01,  1.2327e-01,  3.1905e-01,\n",
       "          1.7882e-02, -4.7733e-01, -4.0570e-01, -9.7906e-02, -3.9830e-01,\n",
       "         -3.2315e-01,  4.5852e-01,  5.6298e-02, -4.6132e-01,  6.3126e-02,\n",
       "         -4.4313e-01, -4.3523e-02, -2.1989e-01,  1.5554e-01, -2.8521e-01,\n",
       "         -1.6950e-01,  4.7483e-02,  1.4223e-01, -4.5080e-01, -2.6366e-01,\n",
       "         -1.5620e-01,  2.4166e-01,  4.4282e-01, -8.2939e-02,  1.1997e-01,\n",
       "         -1.7720e-02,  3.6912e-01,  3.9332e-01,  1.6665e-01,  7.9002e-02,\n",
       "         -1.5330e-01, -1.6126e-01,  3.7063e-01,  7.0298e-02,  1.4623e-01,\n",
       "         -2.4693e-01,  2.5721e-02, -2.3281e-01, -3.1482e-01,  1.4820e-01,\n",
       "          4.2068e-01,  3.2337e-01,  1.6090e-01,  4.8289e-01, -1.3437e-01,\n",
       "         -1.3357e-01, -4.0732e-01,  3.8818e-01, -4.2529e-01, -3.7258e-01,\n",
       "          3.7369e-01, -5.0266e-02, -1.0642e-01,  2.8776e-01,  2.2167e-01,\n",
       "          1.2928e-01, -5.2390e-02, -3.7240e-01, -3.5484e-03,  3.6238e-01,\n",
       "         -4.0364e-02,  2.6802e-01,  1.1419e-01, -7.1792e-02, -4.3012e-01,\n",
       "          3.9256e-01, -1.8475e-01, -4.5062e-01,  1.9000e-01, -1.8650e-04,\n",
       "         -2.0112e-01, -1.5058e-01, -4.3190e-04, -5.1187e-02, -1.6124e-01,\n",
       "         -3.9582e-01,  3.8025e-01, -4.7854e-01, -8.4481e-02, -1.9682e-01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_9.fc.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model_1.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = model_2.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w9 = model_9.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.eq(w1, w9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
