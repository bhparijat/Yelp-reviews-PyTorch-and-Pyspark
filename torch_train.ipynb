{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import time\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import string\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epochs = 10\n",
    "num_class = 2\n",
    "embed_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"gpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewsSentimentAnalysis(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_dim,num_class):\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,json_file,threshold=3):\n",
    "        \n",
    "        self.raw_data = pd.read_json(json_file,lines=True)\n",
    "    \n",
    "        self.raw_data['label'] = self.raw_data.stars.apply(lambda x : 1 if x>=3 else 0)\n",
    "        \n",
    "        self.raw_data = self.raw_data[[\"label\",\"text\"]].iloc[:1000,]\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        \n",
    "        self.idx2word = {}\n",
    "        \n",
    "        self.word2freq = {}\n",
    "        self.word_count = 0\n",
    "        \n",
    "        self.maxLen = 0\n",
    "        self.__init__preprocess()\n",
    "        \n",
    "        self.data = self.raw_data.to_numpy()\n",
    "   \n",
    "        \n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        \n",
    "        sample = self.data[idx,:]\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __init__preprocess(self):\n",
    "        \n",
    "        \n",
    "        def clean(text):\n",
    "            text = text.lower()\n",
    "        \n",
    "            text = [ch for ch in text if ch not in string.punctuation]\n",
    "\n",
    "\n",
    "            text = \"\".join(text)\n",
    "\n",
    "            text = [c for c in text if c == \" \" or c.isalnum()]\n",
    "\n",
    "            text = \"\".join(text)\n",
    "\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "            text = text.split(\" \")\n",
    "\n",
    "            text = [word for word in text if word not in stop_words]\n",
    "        \n",
    "        \n",
    "            text = \" \".join(text)\n",
    "            \n",
    "            return text\n",
    "          \n",
    "            \n",
    "        def build_vocab(text):\n",
    "            \n",
    "            \n",
    "            text = text.split(\" \")\n",
    "            \n",
    "            text_token = []\n",
    "            \n",
    "            for word in text:\n",
    "                \n",
    "                if word not in self.word2idx:\n",
    "                    \n",
    "                    self.word2idx[word] = self.word_count\n",
    "                    \n",
    "                    self.idx2word[self.word_count] = word\n",
    "                    \n",
    "                    self.word_count+=1\n",
    "                    \n",
    "                    \n",
    "                text_token.append(self.word2idx[word])\n",
    "             \n",
    "            self.maxLen = max(self.maxLen,len(text_token))\n",
    "            return text_token\n",
    "        \n",
    "        self.raw_data['text'] = self.raw_data.text.apply(lambda x : clean(x))\n",
    "        self.raw_data['text'] = self.raw_data.text.apply(lambda x : build_vocab(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_offset(batch):\n",
    "    \n",
    "    text = [torch.tensor(x[1]) for x in batch]\n",
    "    \n",
    "    label = torch.tensor([x[0] for x in batch])\n",
    "    offsets = [0] + [len(x) for x in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    \n",
    "    text = torch.cat(text)\n",
    "    \n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(train):\n",
    "    \n",
    "    train_data = DataLoader(train,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_offset,num_workers=4)\n",
    "    \n",
    "    for i, (text, offsets, label) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, label = text.to(device), offsets.to(device), label.to(device)\n",
    "        \n",
    "    \n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0\n",
      " list([3347, 3348, 567, 709, 907, 695, 3125, 3349, 685, 3253, 3350, 291, 3351, 543, 618, 582, 173, 3348, 567, 454, 218, 60, 1523, 701, 1053, 3, 3352, 227, 3353, 3349, 127, 8, 291, 142, 845, 3354])]\n",
      "800\n",
      "time taken 139.04477405548096\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    yelp_dataset = YelpDataset(json_file = \"~/data/yelp/review.json\")\n",
    "    \n",
    "    print(\"Time Taken to for dataset preprocessing is {} minutes...\".format((time.time()-start_time))/60)\n",
    "    \n",
    "    \n",
    "    train_len = int(len(yelp_dataset)*0.8)\n",
    "    \n",
    "    valid_len = int(len(yelp_dataset)*0.1)\n",
    "    \n",
    "    test_len = len(yelp_dataset) - train_len -valid_len\n",
    "    \n",
    "    train,valid,test = random_split(yelp_dataset,[train_len,valid_len,test_len])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Length of training dataset is {}\".format(train_len))\n",
    "          \n",
    "    print(\"Length of validation dataset is {}\".format(valid_len))\n",
    "          \n",
    "      \n",
    "    model = YelpReviewsSentimentAnalysis(yelp_dataset.word_count,embed_dim, num_class)\n",
    "    \n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "          \n",
    "    for epch in range(epochs):\n",
    "          \n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train_func(train)\n",
    "        valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        \n",
    "        print(\"Time Taken to for dataset preprocessing is {} minutes...\".format(mins))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "train_func(train[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[155, 156, 105, 157, 158, 159, 29, 160, 161, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[192, 193, 194, 195, 196, 197, 198, 199, 196, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[220, 221, 86, 222, 223, 224, 225, 226, 192, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "1      1  [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...\n",
       "2      1  [155, 156, 105, 157, 158, 159, 29, 160, 161, 1...\n",
       "3      1  [192, 193, 194, 195, 196, 197, 198, 199, 196, ...\n",
       "4      0  [220, 221, 86, 222, 223, 224, 225, 226, 192, 1..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_dataset.raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
